---
title: "Create mining clusters"
author: "Victor Maus"
date: "11/10/2020"
output: html_document
---

```{r setup, include=FALSE}
library(fastcluster)
library(dbscan)
library(mininglucc)
library(magrittr)
library(progress)
library(parallel)
library(dplyr)
library(sf)
knitr::opts_chunk$set(echo = TRUE)
```

## Read raw data

For details see [00-data-preparation.Rmd]. 

```{r read-data}
derived_data_path <- here::here('analysis', 'data', 'derived_data')

mining_com <- sf::st_read(
  dsn = stringr::str_glue("{derived_data_path}/mining_commodities.gpkg"),
  fid_column_name = "FID_DATASET",
  stringsAsFactors = FALSE,
  query = "SELECT geom, ISO3_CODE FROM mining_commodities") %>% 
  dplyr::mutate(DATASET = "commodities")

mining_pol <- sf::st_read(
  dsn = stringr::str_glue("{derived_data_path}/global_mining_polygons_v1.gpkg"),
  fid_column_name = "FID_DATASET",
  stringsAsFactors = FALSE,
  query = "SELECT geom, ISO3_CODE FROM mining_polygons") %>% 
  dplyr::mutate(DATASET = "polygons")

mining_features <- dplyr::bind_rows(mining_com, mining_pol) %>% 
  dplyr::mutate(FID = dplyr::row_number())
```

## Calculate distance matrix per country

In this step we calculate the distances among all mining features in meters. 
The results are saved to the `output_dir` per each country. 

```{r dist}
output_dir <- stringr::str_glue("{derived_data_path}/{format(Sys.time(), '%Y%m%d%H%M')}")
mining_split <- split(mining_features, mining_features$ISO3_CODE)
pb <- progress::progress_bar$new(total = length(mining_split))
system.time(dist_matrices <- purrr::map_dfr(
  .x = mining_split,
  .f = mininglucc::calc_dist_matrix,
  output_dir = output_dir, 
  pb = pb)
)
```

## Create mining clusters per country 

Below we cluster mining features based on the distance matrix for each country. 
We set the cutting distance to 10 km, see `?fastcluster::hclust` and `?stats::cutree`.
Features further than 10 km from each other are related only if other features connect them,
see [10.1038/s41597-020-00624-w](https://doi.org/10.1038/s41597-020-00624-w).

```{r h-cluster}
h <- units::set_units(10000, m)
dist_files <- dir(stringr::str_glue("{output_dir}/dist_matrix"), full.names = TRUE)
names(dist_files) <- stringr::str_remove_all(basename(dist_files), ".rds")
country_clusters <- parallel::mclapply(names(dist_files), function(f){
  hcluster <- readRDS(dist_files[[f]]) %>% 
    fastcluster::hclust(method = "single") %>% 
    cutree(h = as.numeric(units::set_units(h, m)))
  mining_split[[f]] %>% 
    dplyr::mutate(ID_CLUSTER = hcluster)
}) %>% 
dplyr::bind_rows()

sf::st_write(country_clusters, 
             delete_dsn = TRUE,
             dsn = stringr::str_glue("{output_dir}/country_clusters.geojson"))

sf::st_write(country_clusters, 
             delete_dsn = TRUE,
             dsn = stringr::str_glue("{output_dir}/country_clusters.gpkg"))
```

DBSCAN algorithm works better to cluster mining points and polygons because of 
the heterogeneity of the data. I set `minPts=1` to allow for clusters composed of 
a single feature. The argument `eps` was set to `20000`, so that algorithm is flexible 
to cluster distant mining features. Though, in many cases the clusters are kept small. 
The results were visually checked. 

```{r dbscan-cluster}
eps <- 20000
dist_files <- dir(stringr::str_glue("{output_dir}/dist_matrix"), full.names = TRUE)
names(dist_files) <- stringr::str_remove_all(basename(dist_files), ".rds")
country_clusters <- parallel::mclapply(names(dist_files), function(f){
  dbcluster <- readRDS(dist_files[[f]]) %>% 
    dbscan::dbscan(eps = eps, minPts = 1)
  mining_split[[f]] %>% 
    dplyr::mutate(ID_CLUSTER = dbcluster$cluster)
}) %>% 
dplyr::bind_rows()

sf::st_write(country_clusters,
             layer = "country_clusters",
             delete_dsn = TRUE,
             dsn = stringr::str_glue("{output_dir}/country_dbclusters.geojson"))

sf::st_write(country_clusters, 
             layer = "country_clusters",
             delete_dsn = TRUE,
             dsn = stringr::str_glue("{output_dir}/country_dbclusters.gpkg"))
```
